{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1743823925684,
     "user": {
      "displayName": "Adam Lalani",
      "userId": "16797312885133551950"
     },
     "user_tz": 240
    },
    "id": "Fwe2HH3wiXFH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6372,
     "status": "ok",
     "timestamp": 1743824014332,
     "user": {
      "displayName": "Adam Lalani",
      "userId": "16797312885133551950"
     },
     "user_tz": 240
    },
    "id": "QVUljTMpAn1Y",
    "outputId": "8abbbfb6-c28f-44db-c285-d39a1548f0fc"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1743824018644,
     "user": {
      "displayName": "Adam Lalani",
      "userId": "16797312885133551950"
     },
     "user_tz": 240
    },
    "id": "q7eXO-Qd449C"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/topics-final-project/batch_requests/batch_results.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form Combined Prompt + Response Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in all .jsonl files within batches directory and form into pandas df\n",
    "batch_files = [f for f in os.listdir('batches') if f.endswith('.jsonl')]\n",
    "batch_dataframes = [pd.read_json(os.path.join('batches', f), lines=True) for f in batch_files]\n",
    "prompts = pd.concat(batch_dataframes, ignore_index=True)\n",
    "\n",
    "prompts = prompts.set_index('custom_id')\n",
    "prompts = prompts.drop(columns=['method', 'url'])\n",
    "prompts\n",
    "\n",
    "# Define a function to extract the question from the user's message content\n",
    "def extract_question(body):\n",
    "    try:\n",
    "        # The question is in the user message (index 1) content, inside the 'messages' list\n",
    "        user_content = body['messages'][1]['content']\n",
    "        \n",
    "        # The question appears after \"Question:\" and before \"Answer:\"\n",
    "        # Let's extract just the question part\n",
    "        if \"Question:\" in user_content and \"Answer:\" in user_content:\n",
    "            # Split by \"Question:\" and take the second part\n",
    "            question_part = user_content.split(\"Question:\")[1]\n",
    "            # Split that by \"Answer:\" and take the first part\n",
    "            question_only = question_part.split(\"Answer:\")[0].strip()\n",
    "            return question_only\n",
    "        else:\n",
    "            # If it doesn't follow the expected format, return the whole content\n",
    "            return user_content.strip()\n",
    "    except (KeyError, IndexError, TypeError) as e:\n",
    "        print(f\"Error extracting question: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply the function to extract questions\n",
    "prompts['question'] = prompts['body'].apply(extract_question)\n",
    "\n",
    "prompts = prompts.drop(columns=['body'])\n",
    "prompts.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oojIWsTxC2FK"
   },
   "outputs": [],
   "source": [
    "# Pull in all .jsonl files within batch_results directory and form into pandas df\n",
    "\n",
    "batch_results_path = 'batch_results'\n",
    "batch_results_files = [f for f in os.listdir(batch_results_path) if f.endswith('.jsonl')]\n",
    "\n",
    "# Read all files into a list of dataframes\n",
    "batch_results_dataframes = [pd.read_json(os.path.join(batch_results_path, f), lines=True) for f in batch_results_files]\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "responses = pd.concat(batch_results_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# Set custom_id column as index of combined_df\n",
    "responses = responses.set_index('custom_id')\n",
    "responses = responses.drop(columns=['id', 'error'])\n",
    "responses\n",
    "\n",
    "# Transform the response column to get the answer\n",
    "def extract_answer(response):\n",
    "    try:\n",
    "        return response['body']['choices'][0]['message']['content']\n",
    "    except (KeyError, TypeError, IndexError):\n",
    "        return None\n",
    "\n",
    "# Apply the transformation to each row\n",
    "responses['answer'] = responses['response'].apply(extract_answer)\n",
    "\n",
    "responses = responses.drop(columns=['response'])\n",
    "\n",
    "responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge prompts and responses on custom_id\n",
    "combined_df = pd.merge(prompts, responses, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "print(combined_df.iloc[10]['question'])\n",
    "\n",
    "print(combined_df.iloc[10]['answer'])\n",
    "\n",
    "combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data into Fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer_fragments(answer_text):\n",
    "    if not isinstance(answer_text, str):\n",
    "        return []\n",
    "    \n",
    "    # Split the answer into steps using regex pattern\n",
    "    steps_pattern = r'(Step \\d+:|Conclusion:)'\n",
    "    steps = re.split(steps_pattern, answer_text)\n",
    "    \n",
    "    # Remove any empty strings from the split\n",
    "    steps = [s.strip() for s in steps if s.strip()]\n",
    "    \n",
    "    fragments = []\n",
    "    \n",
    "    # Process each step and extract components\n",
    "    for i in range(0, len(steps), 2):\n",
    "        if i+1 < len(steps):\n",
    "            step_title = steps[i]  # \"Step 1:\", \"Step 2:\", \"Conclusion:\"\n",
    "            step_content = steps[i+1]\n",
    "            \n",
    "            # Special handling for Conclusion\n",
    "            if step_title == \"Conclusion:\":\n",
    "                fragments.append((\"Conclusion:\", step_content))\n",
    "                continue\n",
    "                \n",
    "            # For normal steps, split by semicolon\n",
    "            parts = step_content.split(';', 1)\n",
    "            \n",
    "            if len(parts) == 2:\n",
    "                high_level = parts[0].strip()\n",
    "                low_level = parts[1].strip()\n",
    "                fragments.append((f\"{step_title} {high_level}\", low_level))\n",
    "            else:\n",
    "                fragments.append((f\"{step_title} {step_content}\", \"\"))\n",
    "    \n",
    "    return fragments\n",
    "\n",
    "# Apply the function to split up the answer into chunks containing the full reasoning, including lower-level reasoning (LLR)\n",
    "combined_df['LLR'] = combined_df['answer'].apply(extract_answer_fragments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data\n",
    "combined_df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create High-Level Reasoning (HLR) chunks\n",
    "\n",
    "# Create a function to extract the first element from each tuple in the LLR column\n",
    "def extract_step_titles(llr_tuples):\n",
    "    if not isinstance(llr_tuples, list):\n",
    "        return []\n",
    "    return [item[0] for item in llr_tuples]\n",
    "\n",
    "# Apply the function to create the HLR column\n",
    "combined_df['HLR'] = combined_df['LLR'].apply(extract_step_titles)\n",
    "\n",
    "combined_df.iloc[0,3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('combined_df.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMMe06b03BIPRUmxts7tT8r",
   "mount_file_id": "1Ko5oXeE11d66WEDT4hQ6YuNaH_4gvg7w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
