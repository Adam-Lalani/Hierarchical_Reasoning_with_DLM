See below for a visualization of the distinct training vs. sampling datasets
Note: HLP = 'High Level Planning'; LLR = 'Low Level Reasoning'

Explanation: 
We have two distinct dataset types, those with only HLP and those with both HLP and LLR. 
We also append to the prompt a regular phrase (either 'High-Level Plan' or 'Low-Level Reasoning'), denoted here with [H] and [L] (call this the mode signaler)
During training we clamp the prompt + mode signaler and, if the dataset type has both HLP and LLR, we also clamp the HLP. 

We also enforce a uniform max token length over all examples, using an end-of-answer token (<EOA>) and padding tokens (<PAD>) to fill space up to our token limit. 


Ground Truth: 

PROMPT [H] HLP HLP HLP HLP HLP HLP [L] LLR LLR LLR LLR LLR LLR <EOA> <PAD> ... <PAD>


Training: 
(Training Type 1: High Level Plan Only)
PROMPT [H] HLP HLP HLP HLP HLP HLP <EOA> <PAD> ... ... <PAD>
^clamp: "PROMPT [H]" + "<PAD> ... <PAD>" 

(Training Type 2: High Level Plan  + Low Level Reasoning)
PROMPT [H] HLP HLP HLP HLP HLP HLP [L] LLR LLR LLR LLR LLR LLR <EOA> <PAD> ... <PAD>
^clamp: "PROMPT [H]" + "HLP HLP HLP HLP HLP HLP" + "[L]" + "<PAD> ... <PAD>" 



Sampling: 

(Stage 1: Sample High Level Plan) PROMPT [H] ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___  --> PROMPT [H] HLP HLP HLP HLP HLP HLP <EOA> <PAD> ... ... <PAD>
^mask all tokens after the prompt


(Stage 2: Sample ) PROMPT [H] HLP HLP HLP HLP HLP HLP [L] ___ ___ ___ ___ ___ ___ ... ___--> (2) PROMPT [H]  HLP HLP HLP HLP HLP HLP [L] LLR LLR LLR LLR LLR LLR <EOA> <PAD> ... <PAD>
^Carry over the HLP sampled in stage 1, mask all tokens afterward

