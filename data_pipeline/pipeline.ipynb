{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: combined_df_v2.csv is our initial dataset containing high-level planning and low-level reasoning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "path = \"combined_df_v2.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "# Rename HLR col as HLP\n",
    "df = df.rename(columns={'HLR': 'HLS'})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2TokenizerFast\n",
    "import ast\n",
    "\n",
    "def clean_answer(text):\n",
    "    text = text.replace('\\n\\n', ' ')\n",
    "    text = text.replace('. Step', ' ; Step')\n",
    "    text = text.rstrip('. ')\n",
    "    return text\n",
    "\n",
    "new_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    test_q = row['question']\n",
    "    test_a = row['answer']\n",
    "    test_llr = row['LLR']\n",
    "    test_hlr = row['HLS']\n",
    "\n",
    "    test_q_HL = 'Question: ' + test_q + \" ; \" + ' High Level Steps: '\n",
    "    test_a_HL = ' ; '.join(ast.literal_eval(test_hlr)) + ' '\n",
    "\n",
    "    test_a_LLR = clean_answer(test_a)\n",
    "\n",
    "    test_full_HL = test_q_HL + \" ; \" + test_a_HL\n",
    "    test_full = 'Question: ' + test_q + \" ; \" + ' Answer: ' +  \" ; \" + test_a_LLR\n",
    "\n",
    "    new_rows.append({\"high_level\": test_full_HL, \"low_level\": test_full})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output transformed dataset to data/train_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(new_rows)\n",
    "new_df.head()\n",
    "\n",
    "new_df.to_csv('../data/train_data_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Load data + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "import torch\n",
    "from tqdm.notebook import tqdm  # Use notebook version for better progress bars\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "pad_token_str = tokenizer.pad_token\n",
    "print(f\"Using padding token: '{pad_token_str}'\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "pad_token_str = tokenizer.pad_token\n",
    "print(f\"Using padding token: '{pad_token_str}'\")\n",
    "\n",
    "# Define path relative to the notebook location\n",
    "input_csv_path = '../data/train_data_raw.csv'\n",
    "\n",
    "print(f\"Loading data from {input_csv_path}...\")\n",
    "try:\n",
    "    # Load the raw training data generated by the previous cells/script\n",
    "    eda_df = pd.read_csv(input_csv_path)\n",
    "    # Drop rows with missing values just in case\n",
    "    eda_df.dropna(subset=['high_level', 'low_level'], inplace=True)\n",
    "    print(f\"Loaded {len(eda_df)} rows.\")\n",
    "    print(\"Data sample:\")\n",
    "    display(eda_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at {input_csv_path}\")\n",
    "    print(\"Please ensure the previous cells or the `training_dataset_builder.py` script has run successfully.\")\n",
    "    eda_df = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    eda_df = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing a sample (first 5 rows)...\")\n",
    "sample_df = eda_df.head().copy() # Work on a copy\n",
    "\n",
    "# Ensure data is string type\n",
    "sample_df['high_level'] = sample_df['high_level'].astype(str)\n",
    "sample_df['low_level'] = sample_df['low_level'].astype(str)\n",
    "\n",
    "# Tokenize\n",
    "sample_df['hl_tokens'] = sample_df['high_level'].apply(lambda x: tokenizer.encode(x))\n",
    "sample_df['ll_tokens'] = sample_df['low_level'].apply(lambda x: tokenizer.encode(x))\n",
    "\n",
    "# Get lengths\n",
    "sample_df['hl_len'] = sample_df['hl_tokens'].apply(len)\n",
    "sample_df['ll_len'] = sample_df['ll_tokens'].apply(len)\n",
    "\n",
    "print(\"\\nSample Data with Token Lengths:\")\n",
    "display(sample_df[['high_level', 'hl_len', 'low_level', 'll_len']])\n",
    "\n",
    "print(\"\\nExample Tokenized Output (Row 0):\")\n",
    "print(\"High Level Tokens:\", sample_df.loc[0, 'hl_tokens'])\n",
    "print(\"Decoded:\", tokenizer.decode(sample_df.loc[0, 'hl_tokens'])) # Optional: decode back\n",
    "print(\"\\nLow Level Tokens:\", sample_df.loc[0, 'll_tokens'])\n",
    "print(\"Decoded:\", tokenizer.decode(sample_df.loc[0, 'll_tokens'])) # Optional: decode back\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\nExample Tokenized Output (Row 1):\")\n",
    "print(\"High Level Tokens:\", sample_df.loc[1, 'hl_tokens'])\n",
    "print(\"Decoded:\", tokenizer.decode(sample_df.loc[1, 'hl_tokens'])) # Optional: decode back\n",
    "print(\"\\nLow Level Tokens:\", sample_df.loc[1, 'll_tokens'])\n",
    "print(\"Decoded:\", tokenizer.decode(sample_df.loc[1, 'll_tokens'])) # Optional: decode back\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Split into fragments, get token lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fragment extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "def extract_tokenize_fragments(row_index, high_level_text, low_level_text, tokenizer, delimiter=\" ; \"):\n",
    "    \"\"\"\n",
    "    Extracts HL and LL fragments from texts, tokenizes them,\n",
    "    and returns a list of dictionaries for this row.\n",
    "    Splits \"Conclusion:\" prefix into its own fragment.\n",
    "    \"\"\"\n",
    "    fragments_data = []\n",
    "    # Use separate counters for accurate in-row indexing after potential splits\n",
    "    hl_fragment_output_index = 0\n",
    "    ll_fragment_output_index = 0\n",
    "    conclusion_prefix = \"Conclusion:\" # Define the prefix to split\n",
    "\n",
    "    # --- Process High Level ---\n",
    "    if isinstance(high_level_text, str):\n",
    "        hl_fragments_raw = [frag.strip() for frag in high_level_text.split(delimiter)]\n",
    "        # Using slice [2:] to discard \"Question:\" and \"High Level Steps:\" prefix.\n",
    "        # Change to [1:] if you want to include \"High Level Steps:\" as the first fragment.\n",
    "        hl_step_fragments = [frag for frag in hl_fragments_raw[2:] if frag]\n",
    "\n",
    "        for frag_text in hl_step_fragments:\n",
    "            try:\n",
    "                conclusion_idx = frag_text.find(conclusion_prefix)\n",
    "\n",
    "                # Case 1: \"Conclusion:\" is found mid-fragment\n",
    "                if conclusion_idx > 0:\n",
    "                    part1 = frag_text[:conclusion_idx].strip()\n",
    "                    # Add part 1 if it's not empty\n",
    "                    if part1:\n",
    "                        len1 = len(tokenizer.encode(part1))\n",
    "                        fragments_data.append({\n",
    "                            'original_index': row_index,\n",
    "                            'fragment_index_in_row': hl_fragment_output_index,\n",
    "                            'fragment_type': 'HL',\n",
    "                            'fragment_text': part1,\n",
    "                            'token_length': len1\n",
    "                        })\n",
    "                        hl_fragment_output_index += 1\n",
    "\n",
    "                    # Add \"Conclusion:\" prefix\n",
    "                    len_prefix = len(tokenizer.encode(conclusion_prefix))\n",
    "                    fragments_data.append({\n",
    "                        'original_index': row_index,\n",
    "                        'fragment_index_in_row': hl_fragment_output_index,\n",
    "                        'fragment_type': 'HL',\n",
    "                        'fragment_text': conclusion_prefix,\n",
    "                        'token_length': len_prefix\n",
    "                    })\n",
    "                    hl_fragment_output_index += 1\n",
    "\n",
    "                    # Add remaining text after prefix, if any\n",
    "                    remaining_text = frag_text[conclusion_idx + len(conclusion_prefix):].strip()\n",
    "                    if remaining_text:\n",
    "                        len_remaining = len(tokenizer.encode(remaining_text))\n",
    "                        fragments_data.append({\n",
    "                             'original_index': row_index,\n",
    "                             'fragment_index_in_row': hl_fragment_output_index,\n",
    "                             'fragment_type': 'HL',\n",
    "                             'fragment_text': remaining_text,\n",
    "                             'token_length': len_remaining\n",
    "                        })\n",
    "                        hl_fragment_output_index += 1\n",
    "\n",
    "                # Case 2: Fragment *starts* with \"Conclusion:\"\n",
    "                elif conclusion_idx == 0:\n",
    "                    # Add \"Conclusion:\" prefix\n",
    "                    len_prefix = len(tokenizer.encode(conclusion_prefix))\n",
    "                    fragments_data.append({\n",
    "                        'original_index': row_index,\n",
    "                        'fragment_index_in_row': hl_fragment_output_index,\n",
    "                        'fragment_type': 'HL',\n",
    "                        'fragment_text': conclusion_prefix,\n",
    "                        'token_length': len_prefix\n",
    "                    })\n",
    "                    hl_fragment_output_index += 1\n",
    "\n",
    "                    # Add remaining text after prefix, if any\n",
    "                    remaining_text = frag_text[len(conclusion_prefix):].strip()\n",
    "                    if remaining_text:\n",
    "                        len_remaining = len(tokenizer.encode(remaining_text))\n",
    "                        fragments_data.append({\n",
    "                             'original_index': row_index,\n",
    "                             'fragment_index_in_row': hl_fragment_output_index,\n",
    "                             'fragment_type': 'HL',\n",
    "                             'fragment_text': remaining_text,\n",
    "                             'token_length': len_remaining\n",
    "                        })\n",
    "                        hl_fragment_output_index += 1\n",
    "\n",
    "                # Case 3: No \"Conclusion:\", add as a single fragment\n",
    "                else:\n",
    "                    token_length = len(tokenizer.encode(frag_text))\n",
    "                    fragments_data.append({\n",
    "                        'original_index': row_index,\n",
    "                        'fragment_index_in_row': hl_fragment_output_index,\n",
    "                        'fragment_type': 'HL',\n",
    "                        'fragment_text': frag_text,\n",
    "                        'token_length': token_length\n",
    "                    })\n",
    "                    hl_fragment_output_index += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"Error tokenizing/processing HL fragment at index {row_index}: {e} - Text: {frag_text[:50]}...\")\n",
    "\n",
    "\n",
    "    # --- Process Low Level ---\n",
    "    if isinstance(low_level_text, str):\n",
    "        ll_fragments_raw = [frag.strip() for frag in low_level_text.split(delimiter)]\n",
    "        # Using slice [2:] to discard \"Question:\" and \"Answer:\" prefix.\n",
    "        # Change to [1:] if you want to include \"Answer:\" as the first fragment.\n",
    "        ll_reasoning_fragments = [frag for frag in ll_fragments_raw[2:] if frag]\n",
    "\n",
    "        for frag_text in ll_reasoning_fragments:\n",
    "            try:\n",
    "                 conclusion_idx = frag_text.find(conclusion_prefix)\n",
    "\n",
    "                 # Case 1: \"Conclusion:\" is found mid-fragment\n",
    "                 if conclusion_idx > 0:\n",
    "                     part1 = frag_text[:conclusion_idx].strip()\n",
    "                     # Add part 1 if it's not empty\n",
    "                     if part1:\n",
    "                         len1 = len(tokenizer.encode(part1))\n",
    "                         fragments_data.append({\n",
    "                             'original_index': row_index,\n",
    "                             'fragment_index_in_row': ll_fragment_output_index,\n",
    "                             'fragment_type': 'LL',\n",
    "                             'fragment_text': part1,\n",
    "                             'token_length': len1\n",
    "                         })\n",
    "                         ll_fragment_output_index += 1\n",
    "\n",
    "                     # Add \"Conclusion:\" prefix\n",
    "                     len_prefix = len(tokenizer.encode(conclusion_prefix))\n",
    "                     fragments_data.append({\n",
    "                         'original_index': row_index,\n",
    "                         'fragment_index_in_row': ll_fragment_output_index,\n",
    "                         'fragment_type': 'LL',\n",
    "                         'fragment_text': conclusion_prefix,\n",
    "                         'token_length': len_prefix\n",
    "                     })\n",
    "                     ll_fragment_output_index += 1\n",
    "\n",
    "                     # Add remaining text after prefix, if any\n",
    "                     remaining_text = frag_text[conclusion_idx + len(conclusion_prefix):].strip()\n",
    "                     if remaining_text:\n",
    "                         len_remaining = len(tokenizer.encode(remaining_text))\n",
    "                         fragments_data.append({\n",
    "                              'original_index': row_index,\n",
    "                              'fragment_index_in_row': ll_fragment_output_index,\n",
    "                              'fragment_type': 'LL',\n",
    "                              'fragment_text': remaining_text,\n",
    "                              'token_length': len_remaining\n",
    "                         })\n",
    "                         ll_fragment_output_index += 1\n",
    "\n",
    "                 # Case 2: Fragment *starts* with \"Conclusion:\"\n",
    "                 elif conclusion_idx == 0:\n",
    "                    # Add \"Conclusion:\" prefix\n",
    "                    len_prefix = len(tokenizer.encode(conclusion_prefix))\n",
    "                    fragments_data.append({\n",
    "                        'original_index': row_index,\n",
    "                        'fragment_index_in_row': ll_fragment_output_index,\n",
    "                        'fragment_type': 'LL',\n",
    "                        'fragment_text': conclusion_prefix,\n",
    "                        'token_length': len_prefix\n",
    "                    })\n",
    "                    ll_fragment_output_index += 1\n",
    "\n",
    "                    # Add remaining text after prefix, if any\n",
    "                    remaining_text = frag_text[len(conclusion_prefix):].strip()\n",
    "                    if remaining_text:\n",
    "                        len_remaining = len(tokenizer.encode(remaining_text))\n",
    "                        fragments_data.append({\n",
    "                             'original_index': row_index,\n",
    "                             'fragment_index_in_row': ll_fragment_output_index,\n",
    "                             'fragment_type': 'LL',\n",
    "                             'fragment_text': remaining_text,\n",
    "                             'token_length': len_remaining\n",
    "                        })\n",
    "                        ll_fragment_output_index += 1\n",
    "\n",
    "                 # Case 3: No \"Conclusion:\", add as a single fragment\n",
    "                 else:\n",
    "                     token_length = len(tokenizer.encode(frag_text))\n",
    "                     fragments_data.append({\n",
    "                         'original_index': row_index,\n",
    "                         'fragment_index_in_row': ll_fragment_output_index,\n",
    "                         'fragment_type': 'LL',\n",
    "                         'fragment_text': frag_text,\n",
    "                         'token_length': token_length\n",
    "                     })\n",
    "                     ll_fragment_output_index += 1\n",
    "            except Exception as e:\n",
    "                 print(f\"Error tokenizing/processing LL fragment at index {row_index}: {e} - Text: {frag_text[:50]}...\")\n",
    "\n",
    "\n",
    "    return fragments_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "import torch\n",
    "# Use standard tqdm as notebook version caused issues\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Ensure installed: pip install seaborn\n",
    "\n",
    "# --- Assume tokenizer and eda_df (full or subset) are loaded ---\n",
    "\n",
    "# --- Function Definition (assuming it's defined correctly above) ---\n",
    "# def extract_tokenize_fragments(row_index, high_level_text, low_level_text, tokenizer, delimiter=\" ; \"): ...\n",
    "# --- Verification Step: Process a single example row ---\n",
    "example_row_index = 5 # Choose an index to inspect (e.g., 5)\n",
    "example_fragments_df = None # Initialize DataFrame for the example\n",
    "\n",
    "if eda_df is not None and example_row_index < len(eda_df):\n",
    "    print(f\"\\n--- Verifying fragment extraction for row index: {example_row_index} ---\")\n",
    "    example_row = eda_df.iloc[example_row_index]\n",
    "    example_hl_text = example_row.get('high_level', 'N/A')\n",
    "    example_ll_text = example_row.get('low_level', 'N/A')\n",
    "\n",
    "    # --- Log: Print Original Texts ---\n",
    "    print(\"\\nOriginal High Level Text:\")\n",
    "    print(\"```\")\n",
    "    print(example_hl_text)\n",
    "    print(\"```\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Original Low Level Text:\")\n",
    "    print(\"```\")\n",
    "    print(example_ll_text)\n",
    "    print(\"```\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Call the function just for this row\n",
    "    example_fragments_list = extract_tokenize_fragments(\n",
    "        example_row_index,\n",
    "        example_hl_text,\n",
    "        example_ll_text,\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    print(f\"\\nExtracted Fragments & Details (Row {example_row_index}):\")\n",
    "    if not example_fragments_list:\n",
    "        print(\"  No fragments extracted for this example row.\")\n",
    "    else:\n",
    "        # --- Create DataFrame for the example row's fragments ---\n",
    "        example_fragments_df = pd.DataFrame(example_fragments_list)\n",
    "        print(f\"  Stored fragments for row {example_row_index} in `example_fragments_df`.\")\n",
    "        # Log details from the list (easier to read here than full df display)\n",
    "        for frag_data in example_fragments_list:\n",
    "             print(f\"  - Type: {frag_data['fragment_type']:<3} | \"\n",
    "                   f\"IndexInRow: {frag_data['fragment_index_in_row']:<2} | \"\n",
    "                   f\"Tokens: {frag_data['token_length']:<4} | \"\n",
    "                   f\"Text: '{frag_data['fragment_text'][:80]}...'\") # Truncate text for log clarity\n",
    "    print(\"-\" * 20)\n",
    "    print(\"You can now display `example_fragments_df` manually in the next cell.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Cannot verify example row index {example_row_index}. DataFrame might be empty or index out of bounds.\")\n",
    "\n",
    "# --- Main Processing (The rest of the cell remains the same) ---\n",
    "# print(f\"\\n--- Processing all {len(eda_df)} rows in the current DataFrame ---\")\n",
    "# ... (rest of the code for processing all rows, creating fragments_df, analysis) ...\n",
    "\n",
    "# --- Main Processing: Get all fragments (using the current eda_df, which might be a subset) ---\n",
    "print(f\"\\n--- Processing all {len(eda_df)} rows in the current DataFrame ---\")\n",
    "all_fragments_list = []\n",
    "if eda_df is not None:\n",
    "    # Use tqdm for progress indication\n",
    "    for index, row in tqdm(eda_df.iterrows(), total=len(eda_df), desc=\"Processing Rows\"):\n",
    "        # Add a check for logging the example row *during* the main loop if desired (optional)\n",
    "        # if index == example_row_index:\n",
    "        #    print(f\"\\nProcessing example row {index} within the main loop...\")\n",
    "        row_fragments = extract_tokenize_fragments(\n",
    "            index,\n",
    "            row.get('high_level'), # Use .get for safety\n",
    "            row.get('low_level'),\n",
    "            tokenizer\n",
    "        )\n",
    "        all_fragments_list.extend(row_fragments)\n",
    "\n",
    "    if not all_fragments_list:\n",
    "        print(\"Error: No fragments were extracted during main processing.\")\n",
    "        fragments_df = None\n",
    "        max_len_hl_frag = None\n",
    "        max_len_ll_frag = None\n",
    "    else:\n",
    "        # Create the DataFrame where each row is a single fragment\n",
    "        fragments_df = pd.DataFrame(all_fragments_list)\n",
    "        print(f\"\\nCreated DataFrame with {len(fragments_df)} total fragments.\")\n",
    "        print(\"\\n--- Verifying fragments_df content for example row ---\")\n",
    "        example_fragments_in_df = fragments_df[fragments_df['original_index'] == example_row_index]\n",
    "        if example_fragments_in_df.empty:\n",
    "             print(f\"No fragments found in fragments_df for original_index {example_row_index}\")\n",
    "        else:\n",
    "             print(f\"Fragments stored in DataFrame for original_index {example_row_index}:\")\n",
    "             # Display relevant columns for verification\n",
    "             display(example_fragments_in_df[['original_index', 'fragment_type', 'fragment_index_in_row', 'token_length', 'fragment_text']])\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "\n",
    "        # --- The rest of the analysis (Max Lengths, Visualization, Stats) ---\n",
    "        print(\"\\n--- Calculating Max Lengths and Statistics ---\")\n",
    "        # --- Find Max Lengths by Fragment Type ---\n",
    "        max_lengths = fragments_df.groupby('fragment_type')['token_length'].max()\n",
    "        max_len_hl_frag = max_lengths.get('HL', None)\n",
    "        max_len_ll_frag = max_lengths.get('LL', None)\n",
    "\n",
    "        if max_len_hl_frag is not None: print(f\"Max HL Fragment Token Length: {max_len_hl_frag}\")\n",
    "        else: print(\"No HL fragments found.\")\n",
    "        if max_len_ll_frag is not None: print(f\"Max LL Fragment Token Length: {max_len_ll_frag}\")\n",
    "        else: print(\"No LL fragments found.\")\n",
    "\n",
    "        # --- Visualize Distribution by Type ---\n",
    "        if max_len_hl_frag is not None or max_len_ll_frag is not None:\n",
    "            # (Visualization code remains the same as before)\n",
    "            print(\"\\nVisualizing fragment length distributions...\")\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            if max_len_hl_frag is not None:\n",
    "                 plt.subplot(1, 2, 1)\n",
    "                 sns.histplot(fragments_df[fragments_df['fragment_type'] == 'HL']['token_length'], bins=50)\n",
    "                 plt.title('Distribution of HL Fragment Lengths')\n",
    "                 plt.xlabel('Token Length'); plt.ylabel('Frequency')\n",
    "                 plt.axvline(max_len_hl_frag, color='r', linestyle='--', label=f'Max HL: {max_len_hl_frag}'); plt.legend()\n",
    "            if max_len_ll_frag is not None:\n",
    "                 plt.subplot(1, 2, 2)\n",
    "                 sns.histplot(fragments_df[fragments_df['fragment_type'] == 'LL']['token_length'], bins=50)\n",
    "                 plt.title('Distribution of LL Fragment Lengths')\n",
    "                 plt.xlabel('Token Length')\n",
    "                 plt.axvline(max_len_ll_frag, color='g', linestyle='--', label=f'Max LL: {max_len_ll_frag}'); plt.legend()\n",
    "            if max_len_hl_frag is None or max_len_ll_frag is None:\n",
    "                 plt.subplot(1, 2, 1 if max_len_ll_frag is None else 2).set_ylabel('')\n",
    "                 plt.gcf().set_size_inches(6, 5)\n",
    "            plt.tight_layout(); plt.show()\n",
    "\n",
    "            # Display stats by type\n",
    "            print(\"\\nFragment Length Stats by Type:\")\n",
    "            print(fragments_df.groupby('fragment_type')['token_length'].describe())\n",
    "        else:\n",
    "            print(\"\\nSkipping visualization as no fragments were found.\")\n",
    "else:\n",
    "    print(\"Skipping main processing as eda_df is None.\")\n",
    "    fragments_df = None\n",
    "    max_len_hl_frag = None\n",
    "    max_len_ll_frag = None\n",
    "\n",
    "\n",
    "# --- Explanation for Reconstruction/Prepending ---\n",
    "print(\"\\n--- Note on Reconstructing Sequences ---\")\n",
    "if fragments_df is not None:\n",
    "    print(\"The 'fragments_df' DataFrame now contains all necessary information:\")\n",
    "    print(\"  - 'original_index': Links fragment back to its source row in 'eda_df'.\")\n",
    "    print(\"  - 'fragment_type': Identifies fragment as 'HL' (High Level) or 'LL' (Low Level).\")\n",
    "    print(\"  - 'fragment_index_in_row': Indicates the order of the fragment within its type for that row.\")\n",
    "    print(\"  - 'fragment_text': The actual text content of the fragment.\")\n",
    "    print(\"  - 'token_length': The pre-calculated number of GPT2 tokens.\")\n",
    "    print(\"\\nThis structure allows you to:\")\n",
    "    print(\"  1. Group by 'original_index'.\")\n",
    "    print(\"  2. Within each group, sort fragments by 'fragment_type' and 'fragment_index_in_row' (or any custom order).\")\n",
    "    print(\"  3. Prepend '[{token_length}]' to 'fragment_text'.\")\n",
    "    print(\"  4. Join the modified fragments to reconstruct sequences.\")\n",
    "else:\n",
    "    print(\"Cannot provide reconstruction notes as 'fragments_df' was not created.\")\n",
    "\n",
    "\n",
    "# Update the variables used by subsequent cells\n",
    "max_len_hl = max_len_hl_frag\n",
    "max_len_ll = max_len_ll_frag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments_df[fragments_df['original_index'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create uniform length & length pre-pended datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Ensure installed\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assume eda_df, fragments_df, tokenizer, pad_token_str are available from previous cells\n",
    "# Ensure the 'token_length' column in fragments_df is integer type\n",
    "if 'fragments_df' in locals() and fragments_df is not None:\n",
    "     fragments_df['token_length'] = fragments_df['token_length'].astype(int)\n",
    "else:\n",
    "     print(\"Error: fragments_df not found. Please run previous cells.\")\n",
    "     # Optional: exit or skip\n",
    "     fragments_df = None # Set to None to skip downstream steps cleanly\n",
    "\n",
    "if 'pad_token_str' not in globals() and 'pad_token_str' not in locals():\n",
    "     # Attempt to define it if tokenizer exists\n",
    "     if 'tokenizer' in locals() and tokenizer is not None:\n",
    "         if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "         pad_token_str = tokenizer.pad_token\n",
    "         print(\"Warning: 'pad_token_str' was not found, defined it from tokenizer.\")\n",
    "     else:\n",
    "        raise NameError(\"Execution Error: 'pad_token_str' and 'tokenizer' are not defined. Please run previous cells.\")\n",
    "\n",
    "HL_TARGET_LEN = 35\n",
    "LL_TARGET_LEN = 100\n",
    "OUTPUT_DELIMITER = \" ; \"\n",
    "HL_PLAN_PREFIX = \"High-Level Plan\"\n",
    "LL_REASONING_PREFIX = \"Low-Level Reasoning\"\n",
    "OUTPUT_DIR = \"../data\" # Make sure this directory exists\n",
    "PREPEND_FILENAME = \"train_len_prepend.csv\"\n",
    "UNIFORM_FILENAME = \"train_len_uniform.csv\"\n",
    "\n",
    "# Make sure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Helper Function to Extract Question ---\n",
    "def extract_question(text):\n",
    "    \"\"\"Extracts question text after 'Question: ' and before the first ';'. \"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    try:\n",
    "        q_start = text.find(\"Question: \") + len(\"Question: \")\n",
    "        q_end = text.find(OUTPUT_DELIMITER, q_start)\n",
    "        if q_start < len(\"Question: \") or q_end == -1:\n",
    "             # print(f\"Warning: Could not parse question format: {text[:100]}...\")\n",
    "             return text # Return original text if parsing fails? Or empty string?\n",
    "        return text[q_start:q_end].strip()\n",
    "    except Exception as e:\n",
    "        # print(f\"Error extracting question: {e} from {text[:100]}...\")\n",
    "        return \"\"\n",
    "\n",
    "# --- 1. Extract Original Questions ---\n",
    "if 'eda_df' not in locals() or eda_df is None:\n",
    "     print(\"Error: `eda_df` is not available. Cannot extract questions.\")\n",
    "     question_map = {} # Define as empty to prevent downstream errors\n",
    "else:\n",
    "     print(\"Extracting original questions...\")\n",
    "     question_map = eda_df['high_level'].apply(extract_question).to_dict()\n",
    "     print(f\"Sample question (index 5): {question_map.get(5, 'Not Found')}\")\n",
    "\n",
    "\n",
    "# --- Check if fragments_df exists before proceeding ---\n",
    "if fragments_df is not None:\n",
    "\n",
    "     # --- 2a. Prepare for train_len_prepend ---\n",
    "     print(\"Preparing fragments for train_len_prepend...\")\n",
    "     fragments_df['prepended_text'] = fragments_df.apply(\n",
    "         lambda row: f\"[{row['token_length']}] {row['fragment_text']}\", axis=1\n",
    "     )\n",
    "\n",
    "     # --- 2b. Prepare for train_len_uniform ---\n",
    "     print(\"Preparing fragments for train_len_uniform...\")\n",
    "     # Modified function to accept pad_token\n",
    "     def pad_fragment(row, pad_token):\n",
    "         target_len = HL_TARGET_LEN if row['fragment_type'] == 'HL' else LL_TARGET_LEN\n",
    "         current_len = row['token_length']\n",
    "         pad_needed = max(0, target_len - current_len)\n",
    "         # Removed the check for pad_token_str here\n",
    "         return row['fragment_text'] + (pad_token * pad_needed) # Use the argument\n",
    "\n",
    "     # Modified apply call to pass pad_token_str\n",
    "     fragments_df['padded_text'] = fragments_df.apply(\n",
    "         pad_fragment,\n",
    "         axis=1,\n",
    "         args=(pad_token_str,) # Pass pad_token_str via args\n",
    "     )\n",
    "\n",
    "     # Verify padding for a sample fragment (optional)\n",
    "     if not fragments_df.empty:\n",
    "          sample_padded_frag = fragments_df.iloc[0]\n",
    "          print(f\"Sample padding check (Fragment 0): Type={sample_padded_frag['fragment_type']}, Orig Len={sample_padded_frag['token_length']}\")\n",
    "          # Optional detailed verification can be added here if needed\n",
    "     else:\n",
    "          print(\"Skipping padding check as fragments_df is empty.\")\n",
    "\n",
    "\n",
    "     # --- 3. Group and Reconstruct Sequences ---\n",
    "     tqdm.pandas(desc=\"Building Prepended Sequences\")\n",
    "\n",
    "          # Function to build one sequence row (UPDATED)\n",
    "     def build_sequence(group, text_col_name):\n",
    "          original_index = group.name\n",
    "          question_text = question_map.get(original_index, \"[QUESTION NOT FOUND]\") # Get question\n",
    "\n",
    "          # Get HL fragments (no change needed here)\n",
    "          hl_frags = group[group['fragment_type'] == 'HL'].sort_values('fragment_index_in_row')[text_col_name]\n",
    "\n",
    "          # Get LL fragments BUT filter out the step/conclusion headers\n",
    "          ll_group = group[group['fragment_type'] == 'LL'].sort_values('fragment_index_in_row')\n",
    "\n",
    "          # Filter out rows where fragment_text starts with \"Step \" or is exactly \"Conclusion:\"\n",
    "          # Adjust the startswith('Step ') condition if step numbers can have multiple digits\n",
    "          # and are followed by ':'. A regex might be more robust if format varies.\n",
    "          reasoning_frags_only = ll_group[\n",
    "               ~ll_group['fragment_text'].str.startswith('Step ', na=False) & \\\n",
    "               ~ll_group['fragment_text'].str.startswith('Conclusion:', na=False) # Check against the exact prefix we added\n",
    "          ][text_col_name]\n",
    "\n",
    "          # Join fragments\n",
    "          joined_hl = OUTPUT_DELIMITER.join(hl_frags)\n",
    "          joined_ll_reasoning = OUTPUT_DELIMITER.join(reasoning_frags_only) # Join only the filtered reasoning\n",
    "\n",
    "          # Construct final string\n",
    "          # Format: Question + HL_PREFIX + HL_FRAGS + LL_PREFIX + LL_REASONING_FRAGS_ONLY\n",
    "          parts = [\n",
    "               f\"Question: {question_text}\", # Add \"Question: \" prefix back\n",
    "               HL_PLAN_PREFIX,\n",
    "               joined_hl,\n",
    "               LL_REASONING_PREFIX,\n",
    "               joined_ll_reasoning # Use the filtered LL fragments\n",
    "          ]\n",
    "          # Filter out empty parts that might result if a type had no fragments\n",
    "          parts = [part for part in parts if part] # Especially important if all LL frags were headers\n",
    "          return OUTPUT_DELIMITER.join(parts) \n",
    "\n",
    "     grouped_fragments = fragments_df.groupby('original_index')\n",
    "\n",
    "     print(\"Building prepended sequences...\")\n",
    "     prepended_sequences = grouped_fragments.progress_apply(build_sequence, text_col_name='prepended_text')\n",
    "     prepended_df = pd.DataFrame({'full_sequence': prepended_sequences})\n",
    "     prepended_df.index.name = 'original_index'\n",
    "     print(\"Prepended sequences sample:\")\n",
    "     display(prepended_df.head())\n",
    "\n",
    "     tqdm.pandas(desc=\"Building Uniform Sequences\")\n",
    "     print(\"\\nBuilding uniform length sequences...\")\n",
    "     uniform_sequences = grouped_fragments.progress_apply(build_sequence, text_col_name='padded_text')\n",
    "     uniform_df = pd.DataFrame({'full_sequence': uniform_sequences})\n",
    "     uniform_df.index.name = 'original_index'\n",
    "     print(\"Uniform length sequences sample:\")\n",
    "     display(uniform_df.head())\n",
    "\n",
    "     # --- 4. Save Results ---\n",
    "     prepend_path = os.path.join(OUTPUT_DIR, PREPEND_FILENAME)\n",
    "     uniform_path = os.path.join(OUTPUT_DIR, UNIFORM_FILENAME)\n",
    "\n",
    "     print(f\"\\nSaving prepended data to: {prepend_path}\")\n",
    "     prepended_df.to_csv(prepend_path)\n",
    "\n",
    "     print(f\"Saving uniform length data to: {uniform_path}\")\n",
    "     uniform_df.to_csv(uniform_path)\n",
    "\n",
    "     print(\"\\nProcessing finished.\")\n",
    "\n",
    "else: # This else corresponds to \"if fragments_df is not None:\"\n",
    "     print(\"Skipping sequence building and saving as 'fragments_df' is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/train_len_uniform.csv\"\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10)\n",
    "\n",
    "\n",
    "df.iloc[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up final answer token to avoid latex <-> text ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"../data/train_len_prepend.csv\"\n",
    "df = pd.read_csv(path, index_col = 0)\n",
    "\n",
    "sentence_col = df['full_sequence'] \n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "# extract the last fragment from each row. Delimiter is \" ; \"\n",
    "last_fragment = sentence_col.str.split(' ; ').str[-1]\n",
    "\n",
    "\n",
    "last_fragment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first 4 characters from all rows of last_fragment\n",
    "last_fragment = last_fragment.str[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_fragment\n",
    "\n",
    "# Count # instances which don't contain 'The answer is'. Also count # instances where the sentence ends with a number\n",
    "import pandas as pd\n",
    "count_no_answer_is = (~last_fragment.str.contains('The answer is', na=False)).sum()\n",
    "\n",
    "count_ends_with_number = last_fragment.str.match(r'.*\\d$', na=False).sum()\n",
    "\n",
    "# Of the instances which don't end with a number, print the number of instances which contain '\\(' and '\\)'\n",
    "count_latex_no_number = (~last_fragment.str.match(r'.*\\d$', na=False) & \n",
    "                        last_fragment.str.contains(r'\\\\\\(|\\\\\\)', na=False)).sum()\n",
    "\n",
    "print(f\"Instances without 'The answer is': {count_no_answer_is}\")\n",
    "print(f\"Instances ending with number: {count_ends_with_number}\")\n",
    "print(f\"Instances with LaTeX delimiters but no number: {count_latex_no_number}\")\n",
    "\n",
    "\n",
    "# # Print all instances which don't contain 'The answer is' \n",
    "# count = 0\n",
    "# for frag in last_fragment: \n",
    "#     if ('the answer is' not in frag) and ('The answer is' not in frag):\n",
    "#         print(frag)\n",
    "#         count += 1\n",
    "# print(f\"Number of instances without 'The answer is': {count}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Import regex module\n",
    "\n",
    "# Assuming 'last_fragment' is your Pandas Series \n",
    "# Example Data (replace with your actual Series):\n",
    "# data = [\"The answer is 600\", \"Some text ending with 3\", \"The answer is \\(x^2\\)\", \"Text that ends with \\(y=mx+b\\)\", \"No answer here\", \"the answer is 4.5\", \"Just text\"]\n",
    "# last_fragment = pd.Series(data)\n",
    "\n",
    "# Define regex patterns\n",
    "# Number pattern: Integer or float\n",
    "num_pattern = r'\\d+(\\.\\d+)?' \n",
    "# LaTeX pattern: Matches \\(...\\) non-greedily\n",
    "latex_pattern = r'\\\\\\((.*?)\\\\\\)' \n",
    "\n",
    "# Criterion 1 Checks: Contains \"The answer is\" (case-insensitive) followed by number or LaTeX\n",
    "# Need to be careful with potential spaces after \"is\"\n",
    "regex_c1 = re.compile(\n",
    "    r'the answer is\\s*(' + num_pattern + r'|' + latex_pattern + r')', \n",
    "    re.IGNORECASE # Make the search case-insensitive\n",
    ")\n",
    "criterion1_met = last_fragment.str.contains(regex_c1, na=False)\n",
    "\n",
    "# Criterion 2 Checks: Ends with a number or LaTeX\n",
    "regex_c2_ends_num = re.compile(num_pattern + r'$')\n",
    "regex_c2_ends_latex = re.compile(latex_pattern + r'$')\n",
    "\n",
    "criterion2_ends_num = last_fragment.str.contains(regex_c2_ends_num, na=False)\n",
    "criterion2_ends_latex = last_fragment.str.contains(regex_c2_ends_latex, na=False)\n",
    "criterion2_met = criterion2_ends_num | criterion2_ends_latex\n",
    "\n",
    "# Combine criteria: An answer is identified if EITHER criterion 1 OR criterion 2 is met\n",
    "has_identifiable_answer = criterion1_met | criterion2_met\n",
    "\n",
    "# Get the indices where an answer was identified\n",
    "identified_indices = last_fragment.index[has_identifiable_answer].tolist()\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of entries: {len(last_fragment)}\")\n",
    "print(f\"Number of entries with an identifiable answer: {len(identified_indices)}\")\n",
    "# print(\"\\nIndices with identifiable answers:\")\n",
    "# print(identified_indices) # Uncomment to see the list of indices\n",
    "# print(\"\\nSample entries identified:\")\n",
    "# print(last_fragment[identified_indices].head()) # Uncomment to see sample matching entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter out indices from df which don't have an identifiable answer\n",
    "df = df.iloc[identified_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_frags = last_fragment.iloc[identified_indices] \n",
    "filtered_frags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the latex into human readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sympy as sp\n",
    "from sympy.parsing.latex import parse_latex\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 1: Strip LaTeX wrappers and cosmetic commands\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def strip_latex_wrappers(tex: str) -> str:\n",
    "    tex = tex.strip()\n",
    "    # Remove $...$, \\(...\\), or \\[...\\]\n",
    "    tex = re.sub(r'^\\s*(\\$|\\\\\\(|\\\\\\[)\\s*', '', tex)\n",
    "    tex = re.sub(r'\\s*(\\$|\\\\\\)|\\\\\\])\\s*$', '', tex)\n",
    "    # Remove \\left and \\right\n",
    "    tex = re.sub(r'\\\\left|\\\\right', '', tex)\n",
    "    # Collapse multiple spaces\n",
    "    return \" \".join(tex.split())\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 2: Convert plain 'i'/'j' forms to sympy-compatible 'I'\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "_plain_i = re.compile(\n",
    "    r'(?<![A-Za-z0-9_])'          # not part of identifier\n",
    "    r'([-+]?\\s*\\d*\\.?\\d*(?:/\\d*\\.?\\d*)?\\s*)?'  # optional coeff\n",
    "    r'([ij])'                     # i or j\n",
    ")\n",
    "\n",
    "def plaintext_to_sympy(expr: str) -> str:\n",
    "    def repl(m):\n",
    "        coeff = m.group(1)\n",
    "        return (coeff if coeff and coeff.strip() else '') + ('*I' if coeff else 'I')\n",
    "    return _plain_i.sub(repl, expr)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 3: Parse to SymPy object\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def to_sympy(expr: str):\n",
    "    try:\n",
    "        return parse_latex(expr)\n",
    "    except Exception:\n",
    "        pass\n",
    "    expr2 = plaintext_to_sympy(expr)\n",
    "    try:\n",
    "        return sp.sympify(expr2, rational=True)\n",
    "    except Exception:\n",
    "        # Basic \\frac fallback\n",
    "        m = re.fullmatch(r'\\\\frac\\{([^}]+)\\}\\{([^}]+)\\}', expr)\n",
    "        if m:\n",
    "            return sp.Rational(sp.sympify(m.group(1)), sp.sympify(m.group(2)))\n",
    "        raise\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 4: Canonical form for exact string comparison\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def sympy_canonical(sym):\n",
    "    sym = sp.nsimplify(sym, rational=True)\n",
    "    sym = sp.simplify(sym)\n",
    "    return str(sym)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 5: Master function — pass in string *after* ####\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def normalize_math_answer(raw_ans: str, tol: float = 1e-9) -> str:\n",
    "    stripped = strip_latex_wrappers(raw_ans)\n",
    "    try:\n",
    "        sym = to_sympy(stripped)\n",
    "        # Snap near-zero real or imaginary parts\n",
    "        if (sym.is_real or sym.is_complex):\n",
    "            re_part, im_part = sp.re(sym), sp.im(sym)\n",
    "            if abs(re_part) < tol and abs(im_part) < tol:\n",
    "                sym = sp.Integer(0)\n",
    "        return sympy_canonical(sym)\n",
    "    except Exception:\n",
    "        return stripped  # fallback: cleaned input\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 6: Optional equality checker for eval loop\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def equal_answers(ans1: str, ans2: str, tol: float = 1e-6) -> bool:\n",
    "    try:\n",
    "        a, b = sp.sympify(ans1), sp.sympify(ans2)\n",
    "        return abs(a - b) < tol\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Example usage\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "examples = [\n",
    "    r\"#### \\frac{3}{5}\",\n",
    "    r\"#### \\sqrt{2}\",\n",
    "    r\"#### \\frac{7}{2}i\",\n",
    "    r\"#### i\\sqrt{3}\",\n",
    "    r\"#### 3 + 4i\",\n",
    "    r\"#### -i\",\n",
    "    r\"#### \\frac{\\pi}{6}\",\n",
    "    r\"#### 0.75\",\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    _, raw = ex.split(\"####\")\n",
    "    print(f\"{ex}  -->  {normalize_math_answer(raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "# Make sure sympy is imported if not already done by the script\n",
    "import sympy as sp \n",
    "# Assume the normalization functions (normalize_math_answer, etc.) are defined above\n",
    "\n",
    "# Assuming 'filtered_frags' is your input Pandas Series with original indices preserved.\n",
    "# Example Data (replace with your actual Series):\n",
    "# filtered_frags = pd.Series({\n",
    "#     0: \"The answer is 600\", \n",
    "#     1: \"Some text ending with 3\", \n",
    "#     2: \"The answer is \\(x^2\\)\", \n",
    "#     3: \"Text that ends with \\(y=mx+b\\)\", \n",
    "#     4: \"No answer here\", \n",
    "#     5: \"the answer is 4.5\", \n",
    "#     6: \"Just text ending with 10.\" # Example ending with number + punctuation\n",
    "# })\n",
    "\n",
    "\n",
    "# --- Step 1: Define Regex for Answer Extraction ---\n",
    "\n",
    "# Number pattern: Integer or float (potentially scientific notation)\n",
    "num_pattern = r'[-+]?\\d+(\\.\\d+)?([eE][-+]?\\d+)?' \n",
    "# LaTeX pattern: Matches \\(...\\) non-greedily\n",
    "latex_pattern = r'\\\\\\((.*?)\\\\\\)' \n",
    "\n",
    "# Regex to capture content after \"The answer is\" (case-insensitive)\n",
    "# Captures everything after optional whitespace following \"is\"\n",
    "regex_extract_c1 = re.compile(r'the answer is\\s*(.*)', re.IGNORECASE)\n",
    "\n",
    "# Regex to capture a number at the very end of the string\n",
    "regex_extract_c2_num = re.compile(r'(' + num_pattern + r')\\s*$') # Capture group 1, allow trailing whitespace\n",
    "\n",
    "# Regex to capture LaTeX \\(...\\) at the very end of the string\n",
    "regex_extract_c2_latex = re.compile(r'(' + latex_pattern + r')\\s*$') # Capture group 1, allow trailing whitespace\n",
    "\n",
    "\n",
    "# --- Step 2: Function to Extract Raw Answer String ---\n",
    "\n",
    "def extract_answer(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None # Handle potential non-string data\n",
    "\n",
    "    # Try Criterion 1: Starts with \"The answer is\"\n",
    "    match1 = regex_extract_c1.search(text)\n",
    "    if match1:\n",
    "        # Check if the extracted part *is* actually a number or LaTeX \n",
    "        # This adds robustness based on the original identification criteria\n",
    "        potential_answer = match1.group(1).strip()\n",
    "        if re.fullmatch(num_pattern, potential_answer) or \\\n",
    "           re.fullmatch(latex_pattern, potential_answer):\n",
    "             return potential_answer\n",
    "        # If it starts with \"The answer is\" but isn't followed by num/latex, maybe ignore?\n",
    "        # Or return it anyway? Let's return it based on simpler extraction rule.\n",
    "        # return potential_answer # Option: return even if not strictly num/latex\n",
    "        \n",
    "        # Let's stick closer to the *identification* criteria: only extract if followed by num/latex\n",
    "        # We re-use the *identification* regex here on the *extracted* part\n",
    "        regex_c1_content_check = re.compile(r'^\\s*(' + num_pattern + r'|' + latex_pattern + r')\\s*$')\n",
    "        if regex_c1_content_check.match(potential_answer):\n",
    "             return potential_answer\n",
    "             \n",
    "    # Try Criterion 2: Ends with Number\n",
    "    match2_num = regex_extract_c2_num.search(text)\n",
    "    if match2_num:\n",
    "        return match2_num.group(1).strip() # Group 1 is the captured number\n",
    "\n",
    "    # Try Criterion 2: Ends with LaTeX\n",
    "    match2_latex = regex_extract_c2_latex.search(text)\n",
    "    if match2_latex:\n",
    "        return match2_latex.group(1).strip() # Group 1 is the captured LaTeX\n",
    "\n",
    "    return None # No answer pattern extracted\n",
    "\n",
    "\n",
    "# --- Step 3: Extract Raw Answers ---\n",
    "\n",
    "print(\"Extracting raw answers...\")\n",
    "# Apply the extraction function; result might contain None values\n",
    "raw_answers = filtered_frags.apply(extract_answer)\n",
    "\n",
    "# Filter out entries where no answer could be extracted\n",
    "extracted_answers = raw_answers.dropna()\n",
    "print(f\"Successfully extracted raw answers from {len(extracted_answers)} entries.\")\n",
    "\n",
    "\n",
    "# --- Step 4: Normalize and Format ---\n",
    "\n",
    "print(\"Normalizing extracted answers and formatting...\")\n",
    "final_formatted_answers = {} # Use a dictionary to preserve original indices\n",
    "\n",
    "for index, raw_ans in extracted_answers.items():\n",
    "    # Apply the normalization script's main function\n",
    "    # The normalize_math_answer function includes the stripping logic\n",
    "    try:\n",
    "        normalized_ans = normalize_math_answer(raw_ans)\n",
    "        # Format the output string\n",
    "        final_formatted_answers[index] = f\"The answer is #### {normalized_ans}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error normalizing answer at index {index} ('{raw_ans}'): {e}\")\n",
    "        # Option: Store original or a placeholder if normalization fails\n",
    "        # final_formatted_answers[index] = f\"The answer is #### [Normalization Error: {raw_ans}]\" \n",
    "        final_formatted_answers[index] = f\"The answer is #### {strip_latex_wrappers(raw_ans)}\" # Fallback to just stripped\n",
    "\n",
    "# Convert the dictionary back to a Pandas Series, preserving original indices\n",
    "final_output_series = pd.Series(final_formatted_answers)\n",
    "\n",
    "print(f\"Generated final formatted series with {len(final_output_series)} entries.\")\n",
    "# Display some examples\n",
    "print(\"\\nSample final formatted answers:\")\n",
    "print(final_output_series.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_output_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_last_fragment = df['full_sequence'].str.split(' ; ').str[-1]\n",
    "new_last_fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume 'df' is your original DataFrame\n",
    "# Assume 'final_output_series' is the Series with normalized answers and original indices\n",
    "\n",
    "# --- Step 1: Subset df down to the indices which appear in final_output_series ---\n",
    "\n",
    "# Get the list of indices that are present in the final normalized output\n",
    "valid_indices = final_output_series.index\n",
    "\n",
    "# Filter the original DataFrame to keep only rows with these indices\n",
    "# Use .loc for label-based indexing. No .copy() needed if we modify directly,\n",
    "# but let's create df_subset for clarity like before. Use copy() for safety.\n",
    "df_subset = df.loc[df.index.isin(valid_indices)].copy()\n",
    "\n",
    "print(f\"Original df length: {len(df)}\")\n",
    "print(f\"Subset df length (matching final_output_series indices): {len(df_subset)}\")\n",
    "\n",
    "# --- Step 2: Swap the final fragment using vectorized operations ---\n",
    "\n",
    "# Ensure 'final_output_series' is perfectly aligned with 'df_subset' index\n",
    "# This should already be true, but reindexing or using .loc is safest\n",
    "new_fragments_aligned = final_output_series.loc[df_subset.index]\n",
    "\n",
    "# Perform the right-split on the full sequence column\n",
    "# n=1 ensures we only split on the last occurrence. expand=True creates columns.\n",
    "split_parts = df_subset['full_sequence'].str.rsplit(' ; ', n=1, expand=True)\n",
    "\n",
    "# split_parts[0] contains the prefix (or the full string if no delimiter)\n",
    "# split_parts[1] contains the last fragment (or None if no delimiter)\n",
    "\n",
    "# Identify rows where the delimiter was actually found\n",
    "has_delimiter = split_parts[1].notna()\n",
    "\n",
    "# Construct the result conditionally using np.where\n",
    "# If delimiter exists: prefix + ' ; ' + new_fragment\n",
    "# If delimiter doesn't exist: just the new_fragment\n",
    "df_subset['full_sequence'] = np.where(\n",
    "    has_delimiter, \n",
    "    split_parts[0] + ' ; ' + new_fragments_aligned, # Value if True\n",
    "    new_fragments_aligned                          # Value if False\n",
    ")\n",
    "print(\"\\nReplacement complete using vectorized operations.\")\n",
    "\n",
    "# --- Verification (Optional) ---\n",
    "print(\"\\nVerifying a few examples:\")\n",
    "original_indices_to_check = df_subset.index[:5] # Check the first 5 indices in the subset\n",
    "\n",
    "for idx in original_indices_to_check:\n",
    "    print(f\"\\n--- Index: {idx} ---\")\n",
    "    print(f\"Original Sequence (from df):\")\n",
    "    print(df.loc[idx, 'full_sequence']) \n",
    "    print(f\"New Sequence (from df_subset):\")\n",
    "    print(df_subset.loc[idx, 'full_sequence'])\n",
    "    print(f\"Normalized Fragment Used:\")\n",
    "    # Use .loc to ensure correct alignment when fetching from final_output_series\n",
    "    print(final_output_series.loc[idx]) \n",
    "    \n",
    "# --- Overwrite original df if desired ---\n",
    "# df = df_subset \n",
    "# print(\"\\nOriginal df overwritten with updated subset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index, creating a new column from the old index\n",
    "df_subset_reset = df_subset.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_reset.drop(columns = ['original_index'], inplace = True)\n",
    "df_subset_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv with path data/latex_replaced_data.csv\n",
    "path = \"../data/latex_replaced_data.csv\"\n",
    "\n",
    "df_subset_reset.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorganize train_len_prepend to remove token counters and enforce uniform cumulative token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data:\n",
    "import pandas as pd\n",
    "\n",
    "path = \"../data/latex_replaced_data.csv\"\n",
    "df = pd.read_csv(path, index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instaces of [num] (ex: [7] , [17]) from the full_sequence column\n",
    "df['full_sequence'] = df['full_sequence'].str.replace(r'\\[[0-9]+\\]', '', regex=True)\n",
    "\n",
    "# test instance\n",
    "df['full_sequence'][9996]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "\n",
    "# De-duplicate the df\n",
    "df = df.drop_duplicates()\n",
    "# Re-index\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the distribution of token sequence lengths\n",
    "\n",
    "# from transformers import GPT2TokenizerFast\n",
    "# import matplotlib.pyplot as plt \n",
    "# import numpy as np \n",
    "\n",
    "\n",
    "# sentence_col = df['full_sequence'] \n",
    "\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "# # Tokenize the sentences in batch \n",
    "# # Convert Series to list if necessary\n",
    "# tokenized_output = tokenizer(sentence_col.tolist(), truncation=False) # Don't truncate yet, we want the real lengths\n",
    "\n",
    "# # Get the length of each token sequence using a list comprehension \n",
    "# # This is efficient as the heavy lifting (tokenization) was done in batch\n",
    "# sequence_lengths = [len(ids) for ids in tokenized_output['input_ids']]\n",
    "\n",
    "# # Plot the distribution of sequence lengths\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(sequence_lengths, bins=50, edgecolor='black') # Adjust bins as needed\n",
    "# plt.title('Distribution of Token Sequence Lengths')\n",
    "# plt.xlabel('Sequence Length (Number of Tokens)')\n",
    "# plt.ylabel('Number of Examples')\n",
    "# plt.axvline(x=1024, color='r', linestyle='--', label='GPT-2 Max Length (1024)') # Add line for max length\n",
    "# plt.legend()\n",
    "# plt.grid(axis='y', alpha=0.75)\n",
    "# plt.show() # Display the plot\n",
    "\n",
    "# # Print statistics\n",
    "# print(f\"Maximum sequence length: {max(sequence_lengths)}\")\n",
    "# print(f\"Minimum sequence length: {min(sequence_lengths)}\")\n",
    "# print(f\"Average sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "# num_too_long = sum(1 for length in sequence_lengths if length > 1024)\n",
    "# print(f\"Number of sequences longer than 1024: {num_too_long}\")\n",
    "# print(f\"Percentage of sequences longer than 1024: {(num_too_long / len(sequence_lengths) * 100):.2f}%\")\n",
    "\n",
    "# print(f\"Number of sequences longer than 512: {sum(1 for length in sequence_lengths if length > 512)}\")\n",
    "# print(f\"Percentage of sequences longer than 512: {(sum(1 for length in sequence_lengths if length > 512) / len(sequence_lengths) * 100):.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "# §\n",
    "\n",
    "# Tokenize this and see the number\n",
    "tokenizer.tokenize('Ã')\n",
    "\n",
    "# print the id\n",
    "tokenizer.convert_tokens_to_ids('Ã')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize w/ max length of 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "import numpy as np\n",
    "import torch \n",
    "from tqdm.auto import tqdm # Keep tqdm for potential future loops if needed\n",
    "\n",
    "# --- Setup ---\n",
    "# Assuming 'df' is your initial DataFrame\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "# *** Use a rare existing token for padding ***\n",
    "# Candidate: ID 127 ('Ã') \n",
    "pad_token_to_add = tokenizer.decode([127]) # Get the string 'Ã'\n",
    "tokenizer.add_special_tokens({'pad_token': pad_token_to_add}) \n",
    "\n",
    "# Now, check the pad_token_id that the tokenizer assigned *after* adding\n",
    "pad_token_id = tokenizer.pad_token_id \n",
    "print(f\"Using PAD token ID: {pad_token_id} ('{tokenizer.pad_token}')\") \n",
    "\n",
    "\n",
    "# GPT-2's default EOS token is '<|endoftext|>' with ID 50256\n",
    "eos_token_id = tokenizer.eos_token_id \n",
    "eos_token_string = tokenizer.eos_token\n",
    "print(f\"Using EOS token ID: {eos_token_id} ('{eos_token_string}') for end-of-sequence.\")\n",
    "\n",
    "# Define max length\n",
    "max_len_allowed = 512 \n",
    "\n",
    "# --- Step 1: Clean the text ---\n",
    "# (Previously Step 3)\n",
    "print(\"Step 1: Cleaning text...\")\n",
    "# Operate directly on df, assuming we want to process all rows initially\n",
    "# Create a copy if you want to preserve the original df\n",
    "df_processed = df.copy() \n",
    "df_processed['full_sequence'] = df_processed['full_sequence'].str.replace(r'\\[[0-9]+\\]', '', regex=True)\n",
    "\n",
    "# --- Step 2: Append EOS token string ---\n",
    "# (Previously Step 4, adapted)\n",
    "print(\"Step 2: Appending EOS string...\")\n",
    "# Ensure a space before EOS if tokenizer doesn't handle it automatically\n",
    "df_processed['full_sequence'] = df_processed['full_sequence'] + ' ' + eos_token_string\n",
    "\n",
    "# --- Step 3: Tokenize with Padding and Truncation using Tokenizer ---\n",
    "# (Replaces previous Steps 1, 2, 4, 5, 6)\n",
    "print(f\"Step 3: Tokenizing, padding (with ID {pad_token_id}), and truncating sequences to {max_len_allowed}...\")\n",
    "\n",
    "\n",
    "# # --- Debug ---\n",
    "# print(f\"DEBUG: Tokenizer pad_token_id immediately before call: {tokenizer.pad_token_id}\") \n",
    "# # --------------------\n",
    "    \n",
    "\n",
    "# The tokenizer handles padding with tokenizer.pad_token_id and truncation efficiently.\n",
    "# add_special_tokens=False might be needed if you manually added EOS string,\n",
    "# otherwise, if True, it might add another EOS depending on tokenizer config.\n",
    "# Let's assume manual EOS addition is sufficient.\n",
    "final_tokenization = tokenizer(\n",
    "    df_processed['full_sequence'].tolist(),\n",
    "    padding='max_length',    # Pad to max_length using tokenizer.pad_token_id (now set to 127)\n",
    "    truncation=True,         # Truncate sequences longer than max_length\n",
    "    max_length=max_len_allowed, \n",
    "    add_special_tokens=False,# Set to False as we manually added the EOS string\n",
    "    return_tensors='pt'      # Return PyTorch tensors\n",
    ")\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# --- Final Result ---\n",
    "# 'final_tokenization' now contains 'input_ids' (padded with ID 127) and 'attention_mask'\n",
    "input_ids_tensor = final_tokenization['input_ids']\n",
    "attention_mask_tensor = final_tokenization['attention_mask']\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n",
    "print(f\"Shape of input_ids: {input_ids_tensor.shape}\")\n",
    "print(f\"Shape of attention_mask: {attention_mask_tensor.shape}\")\n",
    "\n",
    "\n",
    "### RUN CHECKS ###\n",
    "\n",
    "# Assuming 'final_tokenization' holds the output from the previous batch \n",
    "# tokenization step (which included padding='max_length', max_length=512)\n",
    "# And 'df' is the filtered and processed DataFrame \n",
    "\n",
    "# Access the tokenized 'input_ids'. The structure depends on 'return_tensors'.\n",
    "input_ids = final_tokenization['input_ids']\n",
    "\n",
    "# Check if the number of sequences matches the DataFrame length\n",
    "assert len(input_ids) == len(df), \\\n",
    "    f\"Verification failed: Mismatch between tokenized sequences ({len(input_ids)}) and DataFrame rows ({len(df)})\"\n",
    "\n",
    "# Check if all sequences have the correct length (512)\n",
    "# If input_ids is a Tensor (PyTorch/TensorFlow):\n",
    "if hasattr(input_ids, 'shape'): \n",
    "    num_sequences, seq_len = input_ids.shape\n",
    "    assert seq_len == 512, \\\n",
    "        f\"Verification failed: Expected sequence length 512, but tensor shape shows {seq_len}\"\n",
    "# If input_ids is a list of lists:\n",
    "else: \n",
    "    all_lengths_correct = all(len(ids) == 512 for ids in input_ids)\n",
    "    assert all_lengths_correct, \\\n",
    "        \"Verification failed: Not all tokenized sequences in the list have length 512.\"\n",
    "\n",
    "print(\"Verification successful: All processed sequences have the target length of 512 tokens.\")\n",
    "\n",
    "# --- Output summary stats and a sample sequence ---\n",
    "print(f\"Number of sequences: {len(df)}\")\n",
    "# Displaying the string sequence might not show the padding added by the tokenizer\n",
    "# print(f\"Sample processed string sequence: {df['full_sequence'].iloc[0]}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"\\nSample tokenization output:\")\n",
    "sample_idx = 100\n",
    "print(f\"Sample input_ids ({sample_idx}): {input_ids[sample_idx].tolist() if hasattr(input_ids, 'tolist') else input_ids[sample_idx]}\")\n",
    "if 'attention_mask' in final_tokenization:\n",
    "    print(f\"Sample attention_mask ({sample_idx}): {final_tokenization['attention_mask'][sample_idx].tolist() if hasattr(final_tokenization['attention_mask'], 'tolist') else final_tokenization['attention_mask'][sample_idx]}\")\n",
    "# decode to see the tokens including special ones\n",
    "print(f\"Decoded sample: {tokenizer.decode(input_ids[sample_idx])}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the fields of the final_tokenization\n",
    "# print(f\"Final tokenization fields: {final_tokenization.keys()}\")\n",
    "\n",
    "# # Compare two different attention masks\n",
    "# print(f\"Attention mask 0: {final_tokenization['attention_mask'][0]}\")\n",
    "# print(f\"Attention mask 1: {final_tokenization['attention_mask'][1]}\")\n",
    "\n",
    "# # Compare two different input IDs\n",
    "# print(f\"Input ID 0: {final_tokenization['input_ids'][0]}\")\n",
    "# print(f\"Input ID 1: {final_tokenization['input_ids'][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df['full_sequence']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized data as a df with input_ids and attention_mask as columns\n",
    "tokenized_df = pd.DataFrame({\n",
    "    'input_ids': final_tokenization['input_ids'].tolist(),\n",
    "    'attention_mask': final_tokenization['attention_mask'].tolist()\n",
    "})\n",
    "\n",
    "tokenized_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate properties of df and tokenized_df, then merge them\n",
    "\n",
    "print(f\"df shape: {df.shape}\")\n",
    "print(f\"tokenized_df shape: {tokenized_df.shape}\")\n",
    "# print(f\"df columns: {df.columns}\")\n",
    "print(f\"tokenized_df columns: {tokenized_df.columns}\")\n",
    "\n",
    "\n",
    "# Check whether the indices are the same\n",
    "if df.index.equals(tokenized_df.index):\n",
    "    print(\"Indices match between df and tokenized_df\")\n",
    "else:\n",
    "    print(\"Indices do not match between df and tokenized_df\")\n",
    "\n",
    "# Observe how many instances there are in which the attention mask does not contain 0\n",
    "print(\"Number of instances in which the attention mask does not contain 0: \", sum(tokenized_df['attention_mask'].apply(lambda x: 0 not in x)))\n",
    "\n",
    "# Of these values, see whether any of them have length of the attention mask less than 512\n",
    "print(\"Number of instances in which the attention mask has length less than 512: \", len(tokenized_df[tokenized_df['attention_mask'].apply(lambda x: len(x) < 512)]))\n",
    "print(\"Number of instances in which the attention mask has length greater than 512: \", len(tokenized_df[tokenized_df['attention_mask'].apply(lambda x: len(x) > 512)]))\n",
    "print(\"Number of instances in which the attention mask has length equal to 512: \", len(tokenized_df[tokenized_df['attention_mask'].apply(lambda x: len(x) == 512)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge the dfs \n",
    "merged_df = pd.concat([df, tokenized_df], axis=1)\n",
    "# Drop the rows where the attention mask does not contain 0\n",
    "merged_df = merged_df[merged_df['attention_mask'].apply(lambda x: 0 in x)]\n",
    "\n",
    "# Print properties of the merged df\n",
    "print(f\"Merged df shape: {merged_df.shape}\")\n",
    "print(f\"Merged df columns: {merged_df.columns}\")\n",
    "\n",
    "# Reset the index of the merged df\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "merged_df.iloc[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check types \n",
    "print(\"full_sequence type: \", type(merged_df.iloc[0,0]))\n",
    "print(\"input_ids type: \", type(merged_df.iloc[0,1]))\n",
    "print(\"attention_mask type: \", type(merged_df.iloc[0,2]))\n",
    "\n",
    "# Print the data types of the columns\n",
    "print(\"data types: \", merged_df.dtypes)\n",
    "\n",
    "type(merged_df.iloc[0,2])\n",
    "\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Check that the attention mask leaves 1 in the spot corresponding to the end of text token (50256). Also check that the next token is 50257 and the next element of the attention mask is 0.\n",
    "\n",
    "# # Mismatching token ids\n",
    "# end_text_token_list = []\n",
    "# next_token_list = []\n",
    "# next_mask_list = []\n",
    "\n",
    "# for row_idx in range(len(merged_df)):\n",
    "#     temp_row = merged_df.iloc[row_idx,:]\n",
    "#     temp_row_sequence = temp_row['full_sequence']\n",
    "#     temp_row_input_ids = temp_row['input_ids']\n",
    "#     temp_row_attention_mask = temp_row['attention_mask']\n",
    "\n",
    "#     # Get the index of the last 1 in the attention mask\n",
    "#     last_one_idx = np.where(np.array(temp_row_attention_mask) == 1)[0][-1]\n",
    "\n",
    "#     # Get the token id at the last 1 in the attention mask\n",
    "#     last_one_token_id = temp_row_input_ids[last_one_idx] \n",
    "\n",
    "#     end_text_token_list.append(last_one_token_id)\n",
    "\n",
    "#     # Get the next token id\n",
    "#     next_token_id = temp_row_input_ids[last_one_idx + 1]\n",
    "#     next_token_list.append(next_token_id)\n",
    "\n",
    "#     # Get the next element of the attention mask\n",
    "#     next_mask_id = temp_row_attention_mask[last_one_idx + 1]\n",
    "#     next_mask_list.append(next_mask_id)\n",
    "\n",
    "\n",
    "# # Print the unique values of the lists\n",
    "# print(\"unique end text token ids: \", np.unique(end_text_token_list))\n",
    "# print(\"unique next token ids: \", np.unique(next_token_list))\n",
    "# print(\"unique next mask ids: \", np.unique(next_mask_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output cleaned dataset (full) and tokenization to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../data/merged_data_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dtypes\n",
    "example_row = merged_df.iloc[0,:] \n",
    "\n",
    "# print type of each col in row\n",
    "for col in example_row.index:\n",
    "    print(f\"{col}: {type(example_row[col])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame\n",
    "\n",
    "def get_question_robust(row):\n",
    "    full_seq = row['full_sequence']\n",
    "    question_text = None # Default value\n",
    "\n",
    "    if isinstance(full_seq, str):\n",
    "        # CORRECTED DELIMITER: Remove the leading space before \"Question:\"\n",
    "        parts_after_marker = full_seq.split(\"Question:\", 1) \n",
    "\n",
    "        if len(parts_after_marker) > 1: # Check if \"Question:\" was found\n",
    "            text_potentially_with_hlp = parts_after_marker[1]\n",
    "            \n",
    "            # Split by \" ; High-Level\" (keep the space here if it exists)\n",
    "            parts_before_hlp = text_potentially_with_hlp.split(\" ; High-Level\", 1)\n",
    "            \n",
    "            # The question is the first part of this second split\n",
    "            question_text = parts_before_hlp[0].strip() \n",
    "        # else: # Optional: Handle cases where \"Question:\" is missing entirely\n",
    "            # print(f\"Warning: 'Question:' not found in row index {row.name}\")\n",
    "            # pass \n",
    "\n",
    "    return question_text\n",
    "\n",
    "# Re-apply the corrected function\n",
    "print(\"Re-extracting questions with corrected delimiter...\")\n",
    "merged_df['question'] = merged_df.apply(get_question_robust, axis=1)\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# Check for rows where extraction might have failed \n",
    "failed_extractions = merged_df['question'].isna().sum()\n",
    "if failed_extractions > 0:\n",
    "    print(f\"Warning: Could not extract question text for {failed_extractions} rows.\")\n",
    "    # Inspect problematic rows if needed:\n",
    "    # print(merged_df[merged_df['question'].isna()][['full_sequence']].head())\n",
    "\n",
    "print(\"\\nDataFrame with 'question' column:\")\n",
    "print(merged_df[['full_sequence', 'question']].head()) # Show relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test instance \n",
    "\n",
    "test_idx = 99756\n",
    "merged_df['question'][test_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = merged_df['question'].unique()\n",
    "\n",
    "# Print the number of unique questions\n",
    "print(f\"Number of unique questions: {len(question_list)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by question and split into train and validation set \n",
    "\n",
    "\n",
    "# First, group by question\n",
    "grouped_df = merged_df.groupby('question')\n",
    "\n",
    "# Then, split into train and validation sets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with the 'question' column populated\n",
    "\n",
    "# --- Step 1: Get Unique Questions ---\n",
    "# Drop rows where question might be None/NaN if extraction failed for some\n",
    "# Or handle them separately if needed\n",
    "unique_questions = merged_df['question'].dropna().unique()\n",
    "print(f\"Found {len(unique_questions)} unique questions.\")\n",
    "\n",
    "# --- Step 2: Split Unique Questions into Train/Validation ---\n",
    "# Define the split ratio \n",
    "validation_size = 0.10\n",
    "# Use a fixed random state for reproducibility\n",
    "random_state = 42 \n",
    "\n",
    "train_questions, val_questions = train_test_split(\n",
    "    unique_questions,\n",
    "    test_size=validation_size,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "print(f\"Splitting into {len(train_questions)} train questions and {len(val_questions)} validation questions.\")\n",
    "\n",
    "# --- Step 3: Create Train and Validation DataFrames ---\n",
    "# Filter the original DataFrame based on the question lists\n",
    "\n",
    "# Create boolean masks\n",
    "is_train_question = merged_df['question'].isin(train_questions)\n",
    "is_val_question = merged_df['question'].isin(val_questions)\n",
    "\n",
    "# Apply masks to create the final DataFrames\n",
    "train_df = merged_df[is_train_question].copy()\n",
    "val_df = merged_df[is_val_question].copy()\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"\\nOriginal DataFrame length: {len(merged_df)}\")\n",
    "print(f\"Train DataFrame length: {len(train_df)}\")\n",
    "print(f\"Validation DataFrame length: {len(val_df)}\")\n",
    "print(f\"Total split length: {len(train_df) + len(val_df)}\")\n",
    "\n",
    "# Optional: Check if any question overlaps between sets (should be empty)\n",
    "train_q_set = set(train_df['question'].unique())\n",
    "val_q_set = set(val_df['question'].unique())\n",
    "overlap = train_q_set.intersection(val_q_set)\n",
    "if not overlap:\n",
    "    print(\"Verification successful: No question overlap between train and validation sets.\")\n",
    "else:\n",
    "    print(f\"Warning: Found {len(overlap)} overlapping questions: {list(overlap)[:5]}...\") # Print first 5 overlaps if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns = ['question'], inplace = True)\n",
    "val_df.drop(columns = ['question'], inplace = True)\n",
    "\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write train and val set to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crate folder if it doesn't exist\n",
    "if not os.path.exists('../data/train_val_sets'):\n",
    "    os.makedirs('../data/train_val_sets')\n",
    "\n",
    "# Write train and val set to CSV\n",
    "train_df.to_csv('../data/train_val_sets/train_set.csv', index = False)\n",
    "val_df.to_csv('../data/train_val_sets/val_set.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
